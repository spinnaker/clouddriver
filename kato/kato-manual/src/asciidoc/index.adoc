= Spinnaker: Project Kato
2013-05-12
:toc: right
:toclevels: 4
:sectanchors:
:sectlink:
:linkattrs:
:numbered:
:appversion: 1.1-SNAPSHOT
:source-highlighter: prettify

== Getting Started

Kato is built as a multi-module Gradle project, which allows its component parts to exist irrespective of one another. This modularity affords for various facets of a cloud deployment pipeline to exist within the context of a single, running service. As such, some modules may depend upon one another, though in most cases, deployment modules should strictly depend on Kato's core module, which provides interfaces for deployment activities and name-keyed credential packs.

Kato comes pre-configured to work with IntelliJ IDEA. As such, the Gradle project will configure itself with appropriate IDEA workspace and project configuration files. Included in this configuration is the preferred per-project code style, the appropriate JDK language level settings, and the Netflix open source Apache License headers on all relevant files. To get started with the project in IntelliJ IDEA, simply execute the +gradle idea+ command in the project's top level.

=== Building

Kato can be built via command-line with either vanilla Gradle, or with Netflix's custom Nebula Gradle wrapper. While the latter is preferred, Kato has no strict requirement for this as part of the build process. To build, simply execute the +gradle build+ command in the project's root. This will produce a set of jar files in the +build/libs+ directories of the sub-projects. Kato's _web_ module specifies the Spring Boot Gradle plugin, so its jar file will be built as a so-called "fat-jar", capable of being run as a standalone archive.

Once built, the +web+ module may be used to act as a service endpoint, or the individuals project artifacts (including +web+) can be included as libraries in another web project.

=== Running

At its most fundamental level, Kato is but a set of libraries. Even its web tier is little more than a few controllers, which are capable of being mapped into another Spring project. This fact gives Kato a great deal of flexibility in _how_ it will be used in its implementation. The web subproject, however, is capable of acting as a standalone server, which bootstraps a Spring Application context within an embedded servlet container. By creating a run configuration within your IDE for the +com.netflix.spinnaker.kato.Main+ class, which is located within the +kato-web+ project module, you can run Kato in its embedded form.

In itself, however, the web project won't offer much functionality besides standing up an in-memory +NamedAccountCredentialsHolder+, which is responsible for the management and distribution of account credentials, bootstrapping +kato-core+'s no-operation deployment handler, and exposing the RESTful HTTP endpoints for the listing of available account credentials, the execution of atomic operations, and the retrieval of tasks that are being, or have been, executed.

== REST API

Kato provides a set of RESTful HTTP APIs, which expose its deployment-related functions (+/ops+), access to tasks that have run or are currently running (+/task+), and access to the list of configured account names, which can be used to specify an account or environment with which a deployment-related activity should take place.

=== Operations

Kato's REST API is driven off the premise that clients will provide a set of "descriptions" for the atomic operations which they wish to execute. Internally, POST'ed "descriptions" will be coerced to atomic operations, which will execute some deployment-related action. The service provides a RESTful HTTP endpoint at +/ops+, which takes an array of objects, where each object is keyed by the name of the operation that it is describing. For example, the request body shown in Listing 1.1 could be supplied to invoke the creation of an Amazon Elastic Load Balancer, the insertion of a DNS record at Route53, and the deployment of a given AMI to Amazon.

.Listing 1.1
[source,javascript]
----
POST /ops
[{
    "upsertAmazonLoadBalancerDescription": {
        "clusterName": "kato-main",
        "subnetType": "internal",
        "securityGroups": ["nf-infrastructure-vpc", "nf-datacenter-vpc"],
        "availabilityZones": {
            "us-east-1": []
        },
        "listeners": [{
            "externalProtocol": "TCP",
            "internalProtocol": "TCP",
            "externalPort": "7001",
            "internalPort": "7001"
        }],
        "healthCheck": "HTTP:7001/health",
        "credentials": "test"
    }
}, {
    "upsertAmazonDNSDescription": {
        "type": "CNAME",
        "name": "kato.test.netflix.net.",
        "hostedZoneName": "test.netflix.net.",
        "credentials": "test"
    }
}, {
    "basicAmazonDeployDescription": {
        "application": "kato",
        "amiName": "ami-xxxxxxxx",
        "stack": "main",
        "instanceType": "m1.medium",
        "securityGroups": ["nf-infrastructure-vpc", "nf-datacenter-vpc"],
        "subnetType": "internal",
        "availabilityZones": {
            "us-east-1": []
        },
        "capacity": {
            "min": 1,
            "max": 1,
            "desired": 1
        },
        "credentials": "test"
    }
}]
----

In this example, +createAmazonLoadBalancerDescription+, +upsertAmazonDNSDescription+, and +basicAmazonDeployDescription+ are the names of the descriptions to which Kato can locate corresponding atomic operations. The result from the above call will nearly always be a data structure that provides a RESTful HTTP reference link to a +Task+ resource, which can be polled by clients to see the status or history of a set of work they have submitted.

The response from a call to the operations endpoint will look like the JSON depicted in Listing 1.2.

.Listing 1.2
[source,javascript]
----
{ "id": "e1jbn3", "resourceUri": "/task/e1jbn3" }
----

It is the "resourceLink" piece of this response that defines the endpoint that clients can use to query the status of their job execution.

==== Amazon Operations

NOTE: Kato's AWS module (+kato-aws+) provides operations that are relevant for deployments and deployment-related function with Amazon.

- - -
include::src/asciidoc/ops/allowLaunchDescription.adoc[]
- - -
include::src/asciidoc/ops/basicAmazonDeployDescription.adoc[]
- - -
include::src/asciidoc/ops/copyLastAsgDescription.adoc[]
- - -
include::src/asciidoc/ops/createNetworkInterfaceDescription.adoc[]
- - -
include::src/asciidoc/ops/deleteAsgDescription.adoc[]
- - -
include::src/asciidoc/ops/deleteAmazonLoadBalancerDescription.adoc[]
- - -
include::src/asciidoc/ops/deleteSecurityGroupDescription.adoc[]
- - -
include::src/asciidoc/ops/registerInstancesWithLoadBalancerDescription.adoc[]
- - -
include::src/asciidoc/ops/deregisterInstancesFromLoadBalancerDescription.adoc[]
- - -
include::src/asciidoc/ops/destroyAsgDescription.adoc[]
- - -
include::src/asciidoc/ops/enableAsgDescription.adoc[]
- - -
include::src/asciidoc/ops/disableAsgDescription.adoc[]
- - -
include::src/asciidoc/ops/enableInstancesInDiscoveryDescription.adoc[]
- - -
include::src/asciidoc/ops/disableInstancesInDiscoveryDescription.adoc[]
- - -
include::src/asciidoc/ops/resumeAsgProcessesDescription.adoc[]
- - -
include::src/asciidoc/ops/suspendAsgProcessesDescription.adoc[]
- - -
include::src/asciidoc/ops/resizeAsgDescription.adoc[]
- - -
include::src/asciidoc/ops/modifyAsgDescription.adoc[]
- - -
include::src/asciidoc/ops/shrinkClusterDescription.adoc[]
- - -
include::src/asciidoc/ops/upsertAsgTagsDescription.adoc[]
- - -
include::src/asciidoc/ops/deleteAsgTagsDescription.adoc[]
- - -
include::src/asciidoc/ops/upsertAsgScheduledActionsDescription.adoc[]
- - -
include::src/asciidoc/ops/rebootInstancesDescription.adoc[]
- - -
include::src/asciidoc/ops/terminateInstanceAndDecrementAsgDescription.adoc[]
- - -
include::src/asciidoc/ops/terminateInstancesDescription.adoc[]
- - -
include::src/asciidoc/ops/upsertAmazonDNSDescription.adoc[]
- - -
include::src/asciidoc/ops/upsertAmazonLoadBalancerDescription.adoc[]
- - -
include::src/asciidoc/ops/upsertScalingPolicyDescription.adoc[]
- - -
include::src/asciidoc/ops/upsertSecurityGroupDescription.adoc[]
- - -

==== Cloud Foundry Operations

NOTE: Kato's CF module (+kato-cf+) provides operations that are relevant for deployments and deployment-related function with any instance of http://www.cloudfoundry.org/index.html[Cloud Foundry].

- - -
include::src/asciidoc/ops/cf/cloudFoundryDeployDescription.adoc[]
- - -

==== Google Operations

TODO

==== Docker Operations

NOTE: Kato's Docker module (+kato-docker+) provides operations that are relevant for deployments and deployment-related function with a Docker host.

- - -
include::src/asciidoc/ops/docker/dockerDeployDescription.adoc[]
- - -
include::src/asciidoc/ops/docker/restartContainerDescription.adoc[]
- - -
include::src/asciidoc/ops/docker/stopContainerDescription.adoc[]
- - -

==== Azure Operations

TODO

==== Rackspace Operations

TODO

==== SoftLayer Operations

TODO

=== Account Configuration

Kato uses the account repository and credentials provider paradigm exposed by http://github.com/spinnaker/amos[Amos].

==== Amazon

===== Single Amazon Account Configuration

Kato's Amazon integration allows for two types of account configuration; the first, where a http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html[DefaultAWSCredentialsProviderChain] can be used to resolve pre-configured credentials for a _single_ account. This is the simplest setup and requires nothing more than following the directions to export your AWS account credentials as environment variables, Java System properties, or through Amazon's other conventional structure.

===== Multiple Amazon Account Configuration

Kato, like all Spinnaker services, is capable of managing a cross-account cloud presence. With respect to integration at AWS, Kato makes use of http://docs.aws.amazon.com/IAM/latest/UserGuide/roles-assume-role.html[Assume Role] configuration to allow a set of pre-configured credentials to manage multiple accounts. Given that, a set of "master" account credentials must be resolvable through the http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html[DefaultAWSCredentialsProviderChain], and Kato will utilize the configuration outlined to apply the appropriate _assume role_ conditions to each AWS call.

====== Listing: Example Cross Account Configuration

[source,yaml]
----
aws:
  regions:
   - eu-west-1
   - us-east-1
   - us-west-1
   - us-west-2
  accountIamRole: SpinnakerInstanceProfile
  assumeRole: role/spinnaker
  defaults:
    iamRole: BaseIAMRole
    keyPair: test-keypair-a
  accounts:
    - name: test
      accountId: 000000
      regions:
        - name: us-east-1
          availabilityZones:
          - us-east-1a
          - us-east-1b
          - us-east-1c
          - us-east-1d
          - us-east-1e
    - name: prod
      accountId: 123456
      regions:
        - name: us-east-1
          availabilityZones:
          - us-east-1a
          - us-east-1b
          - us-east-1c
          - us-east-1d
          - us-east-1e
----

In the example cross-account configuration can be made available to Kato by placing it on the root of the classpath in a file named +application.yml+.
Alternatively, the configuration can be resolved from the file system by supplying the +-Dspring.config.location=/path/to/config.yml+ directive as a Java System property to Kato's startup script.
In the above example, the +aws.regions+ keys informs Kato as to the regions in which the application is capable of operating. Kato will use this detail during validation to ensure the application is able to deploy to a specified target.
The +aws.assumeRole+ directive is what Kato will use to construct the _assume role_ calls that it will use to traverse accounts. In order to facilitate the cross-account presence, accounts must be configured with thier account Ids, under the +aws.accounts+ collection.
These objects are to conform to the object structure as defined by the +NetflixAssumeRoleAmazonCredentials+ object.

==== Google

TODO

==== Docker

A Docker account configuration must come with a configured URL to the host's API, and a link to a private http://blog.docker.com/2013/07/how-to-use-your-own-registry/[registry].
The registry will be used to resolve applications to their corresponding images and image versions. Given that, the public docker repository will be insufficient as a configuration directive,
since we cannot query its search API directly; accounts _must_ be configured with a private registry.

===== Example Docker Account Configuration

[source,yaml]
----
docker:
  accounts:
    - name: local
      url: http://localhost:2375
      registry: http://localhost:5000
    - name: remote
      url: http://remote-host:2375
      registry: http://remote-registry:5000
----

Docker accounts are configured under the +docker.accounts+ key, and must be made available to Kato by placing them on the root of the classpath in a file named +application.yml+.
Alternatively, the configuration can be resolved off the filesystem by providing the +-Dspring.config.location=/path/to/config.yml+ Java System property to Kato's startup script.

=== Tasks

NOTE: The endpoint mapped to +/task+ provides details about tasks that Kato has run or is running. When a description is submitted to Kato, it is transformed into an atomic operation and delegated to an orchestration engine, which will (should) process the request in the background, while returning a +Task+ object to the foreground. A response from a post to the operations controller will include a "resourceLink" to the task. That resource link can, in turn, be queried by clients to observe the current status of the submitted execution. An example output of a +DefaultTask+ object might look like the response shown in Listing 1.3.

==== Endpoints

[width="100%",frame="topbot",options="header,footer"]
|======================
|Request Type | Endpoint    | Description
|GET          | /task       | Will return a list of all running and previously run tasks.
|GET          | /task/:id   | Will return the task for the specified ID.
|======================

==== Example

.Listing 1.3
[source,javascript]
----
GET /task/e1jbn3
{
    "id": "e1jbn3",
    "phase": "AWS_DEPLOY",
    "status": "Deploying new AutoScaling group...",
    "complete": false,
    "failed": false,
    "startTimeMs": 1400011488500,
    "history": [{
        "phase": "ORCHESTRATION",
        "status": "Beginning orchestration"
    }, {
        "phase": "ORCHESTRATION",
        "status": "Invoking AWS Deploy handler..."
    }, {
        "phase": "AWS_DEPLOY",
        "status": "Deploying new AutoScalingGroup...."
    }]
}
----

=== Credentials

NOTE: The endpoint mapped to +/credentials+ provides the names of the accounts that are configured with Kato, along with the cloud provider each account is associated with. Descriptions that are POST'ed to the +/ops+ endpoint may require a credential's name to resolve it internally.

==== Endpoints

[width="100%",frame="topbot",options="header,footer"]
|======================
|Request Type | Endpoint            | Description
|GET          | /credentials        | Returns an array of maps describing the configured credential account names and provider types
|GET          | /credentials/:name  | Returns the details of the specified credentials
|======================

==== Example

.Listing 1.4
[source,javascript]
----
GET /credentials
[
  {
    name: "test",
    type: "aws"
  },
  {
    name: "prod",
    type: "gce"
  }
]

GET /credentials/test
{
  regions: [
    {
      name: "us-east-1",
      availabilityZones: [
        "us-east-1c",
        "us-east-1d",
        "us-east-1e"
      ]
    }
  ]
}
----

== Copyrights and Licenses

Copyright (C) 2014 Netflix. Licensed under the Apache License.

See https://raw.githubusercontent.com/spinnaker/kato/master/LICENSE.txt[LICENSE.txt] for more information.
